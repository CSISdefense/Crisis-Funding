# DOD BUDGET PDFS AUTOMATIC DOWNLOAD PROJECT
# April 2016

################################################################################
# THIS SCRIPT WILL DOWNLOAD AND SAVE ALL PDFS FROM LISTS:
# - PDFURLs.txt         - the web locations of PDFs to download 
# - PDFNames.txt        - the path and file names for saving the PDFs locally
#
# BOTH LISTS ARE AUTO-GENERATED BY THE PrepareDownload.R SCRIPT IN THIS FOLDER;
# USE THAT SCRIPT FIRST IF YOU WANT TO UPDATE THE LISTS OF PDFs TO DOWNLOAD
################################################################################

library(stringr)
library(httr)

# Save original working directory
originalwd <- getwd()

# Set location for downloads
setwd("K:/Development/Budget/Budget Materials PDF download")

################################################################################
# READ WEBPAGES
# Build a dataframe of:
# - urls of pdfs found on each webpage
# - the source of each URL
# - file path for output
################################################################################

# pagelist <- as.character(unlist(read.delim("pagelist.txt")))
pagelist <- as.character(unlist(read.delim("test_pagelist.txt")))
URLsToSave <- vector("character", length = 0)
PathsToSave <- vector("character", length = 0)

for(i in seq_along(pagelist)) {
      url <- as.character(pagelist[i])
      html = GET(url)
      contents = content(html, as="text")

      # Build list of the URLs of all PDFs linked within the read website
      pdfs <- unlist(str_extract_all(contents,
                                     '/[A-Za-z0-9-._~:/?#@!%$&()*+,;=]*?\\.pdf'))
      # Note, I am excluding  ', [, ], which are each valid.

      
### Create "from" and "pagename" vectors to determine file structure when saving
### "from" : which agency produced the PDF
### "pagename": which page was the PDF found on
      if(grep("comptroller.defense.gov", url)){
            pdfurl <- paste("http://comptroller.defense.gov", pdfs, sep="")
            from <- "Comptroller"
            pagename <- unlist(strsplit(
                  url,"http://comptroller.defense.gov/BudgetMaterials/"))[2]
            pagename <- unlist(strsplit(pagename, "\\.aspx"))[1]
      }
### Create file structure
      dir.create(from, showWarnings = FALSE)
      dir.create(file.path(from, pagename), showWarnings = FALSE)
      
      
### Create the individual file name for the PDF as it will be saved
      pdfname <- str_extract(pdfurl, "/[A-Za-z0-9-._~:?#@!%$&()*+,;=]*?\\.pdf")
      
### Combine the agency, page, and file names to create the entire filepath
      savepath <- vector("character", length = 0)
      for(j in seq_along(pdfurl)){
            savepath[j] <- paste(from, "/", pagename, pdfname[j],
                                 sep = "")
      }

      PathsToSave <- append(PathsToSave, savepath)
      URLsToSave <- append(URLsToSave, pdfurl)
         
      
}




################# STOP HERE ##########################

download.file(pdfurl, savepath, method = "libcurl", mode="wb")  


otherpages <- str_extract_all(contents, '/[A-Za-z0-9-._~:/?#@!$&)*+,;=]*?aspx' )
otherpages <- paste("http://comptroller.defense.gov", unlist(otherpages), sep ="")
pagelist <- grep("BudgetMaterials", otherpages) 


write(PathsToSave, file = "paths.txt")



# Cleanup: return to original working directory
setwd(originalwd)